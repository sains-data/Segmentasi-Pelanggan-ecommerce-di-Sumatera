version: '3'

services:
  namenode:
    build:
      context: .
      dockerfile: docker-compose/hadoop/Dockerfile
    image: hadoop-ecommerce-hadoop:latest
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"   # NameNode web UI
      - "8020:8020"   # NameNode IPC
    environment:
      - NODE_TYPE=namenode
    volumes:
      - ./data:/data
      - ./logs/hadoop/namenode:/home/hadoop/hadoop/logs
    networks:
      - hadoop_network

  resourcemanager:
    build:
      context: .
      dockerfile: docker-compose/hadoop/Dockerfile
    image: hadoop-ecommerce-hadoop:latest
    container_name: resourcemanager
    hostname: resourcemanager
    ports:
      - "8088:8088"   # ResourceManager web UI
      - "8032:8032"   # ResourceManager IPC
    environment:
      - NODE_TYPE=resourcemanager
    volumes:
      - ./logs/hadoop/resourcemanager:/home/hadoop/hadoop/logs
    depends_on:
      - namenode
    networks:
      - hadoop_network

  datanode1:
    build:
      context: .
      dockerfile: docker-compose/hadoop/Dockerfile
    image: hadoop-ecommerce-hadoop:latest
    container_name: datanode1
    hostname: datanode1
    environment:
      - NODE_TYPE=datanode
    volumes:
      - ./logs/hadoop/datanode1:/home/hadoop/hadoop/logs
    depends_on:
      - namenode
    networks:
      - hadoop_network

  datanode2:
    build:
      context: .
      dockerfile: docker-compose/hadoop/Dockerfile
    image: hadoop-ecommerce-hadoop:latest
    container_name: datanode2
    hostname: datanode2
    environment:
      - NODE_TYPE=datanode
    volumes:
      - ./logs/hadoop/datanode2:/home/hadoop/hadoop/logs
    depends_on:
      - namenode
    networks:
      - hadoop_network

  datanode3:
    build:
      context: .
      dockerfile: docker-compose/hadoop/Dockerfile
    image: hadoop-ecommerce-hadoop:latest
    container_name: datanode3
    hostname: datanode3
    environment:
      - NODE_TYPE=datanode
    volumes:
      - ./logs/hadoop/datanode3:/home/hadoop/hadoop/logs
    depends_on:
      - namenode
    networks:
      - hadoop_network

  datanode4:
    build:
      context: .
      dockerfile: docker-compose/hadoop/Dockerfile
    image: hadoop-ecommerce-hadoop:latest
    container_name: datanode4
    hostname: datanode4
    environment:
      - NODE_TYPE=datanode
    volumes:
      - ./logs/hadoop/datanode4:/home/hadoop/hadoop/logs
    depends_on:
      - namenode
    networks:
      - hadoop_network

  spark-master:
    build:
      context: .
      dockerfile: docker-compose/spark/Dockerfile
    image: hadoop-ecommerce-spark:latest
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"   # Spark Master web UI
      - "7077:7077"   # Spark Master port
    environment:
      - NODE_TYPE=master
    volumes:
      - ./scripts:/scripts
      - ./logs/spark/master:/home/hadoop/spark/logs
    depends_on:
      - namenode
      - resourcemanager
    networks:
      - hadoop_network

  spark-worker1:
    build:
      context: .
      dockerfile: docker-compose/spark/Dockerfile
    image: hadoop-ecommerce-spark:latest
    container_name: spark-worker1
    hostname: spark-worker1
    ports:
      - "8081:8081"   # Spark Worker web UI
    environment:
      - NODE_TYPE=worker
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./logs/spark/worker1:/home/hadoop/spark/logs
    depends_on:
      - spark-master
    networks:
      - hadoop_network

  spark-worker2:
    build:
      context: .
      dockerfile: docker-compose/spark/Dockerfile
    image: hadoop-ecommerce-spark:latest
    container_name: spark-worker2
    hostname: spark-worker2
    ports:
      - "8082:8081"   # Spark Worker web UI
    environment:
      - NODE_TYPE=worker
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./logs/spark/worker2:/home/hadoop/spark/logs
    depends_on:
      - spark-master
    networks:
      - hadoop_network

  hive-server:
    build:
      context: .
      dockerfile: docker-compose/hive/Dockerfile
    image: hadoop-ecommerce-hive:latest
    container_name: hive-server
    hostname: hive-server
    ports:
      - "10000:10000"  # HiveServer2 Thrift
      - "10002:10002"  # HiveServer2 Web UI
    environment:
      - NODE_TYPE=server
    volumes:
      - ./scripts:/scripts
      - ./logs/hive:/home/hadoop/hive/logs
    depends_on:
      - namenode
      - resourcemanager
    networks:
      - hadoop_network

  hive-metastore-db:
    image: mysql:8.0
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    ports:
      - "3306:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=hadoop
      - MYSQL_DATABASE=metastore
      - MYSQL_USER=hive
      - MYSQL_PASSWORD=hive
    volumes:
      - ./data/mysql:/var/lib/mysql
    networks:
      - hadoop_network

  nifi:
    build:
      context: .
      dockerfile: docker-compose/nifi/Dockerfile
    image: hadoop-ecommerce-nifi:latest
    container_name: nifi
    hostname: nifi
    ports:
      - "8443:8443"   # NiFi web UI (HTTPS)
    environment:
      - NIFI_WEB_HTTPS_PORT=8443
    volumes:
      - ./data:/data
      - ./logs/nifi:/home/hadoop/nifi/logs
    depends_on:
      - namenode
    networks:
      - hadoop_network

  airflow-webserver:
    build:
      context: .
      dockerfile: docker-compose/airflow/Dockerfile
    image: hadoop-ecommerce-airflow:latest
    container_name: airflow-webserver
    hostname: airflow-webserver
    ports:
      - "8180:8080"   # Airflow web UI
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW_COMPONENT=webserver
    volumes:
      - ./dags:/home/hadoop/airflow/dags
      - ./logs/airflow:/home/hadoop/airflow/logs
    depends_on:
      - namenode
      - spark-master
      - hive-server
    networks:
      - hadoop_network

  airflow-scheduler:
    build:
      context: .
      dockerfile: docker-compose/airflow/Dockerfile
    image: hadoop-ecommerce-airflow:latest
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW_COMPONENT=scheduler
    volumes:
      - ./dags:/home/hadoop/airflow/dags
      - ./logs/airflow:/home/hadoop/airflow/logs
    depends_on:
      - airflow-webserver
    networks:
      - hadoop_network

  superset:
    image: apache/superset:latest
    container_name: superset
    hostname: superset
    ports:
      - "8088:8088"   # Superset web UI
    environment:
      - SUPERSET_SECRET_KEY=superset_secret
    volumes:
      - ./config/superset:/app/superset_config.py
    depends_on:
      - hive-server
    networks:
      - hadoop_network

networks:
  hadoop_network:
    driver: bridge